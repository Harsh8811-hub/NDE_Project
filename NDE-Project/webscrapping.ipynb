{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install PyPDF2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tLxdgNP0qmHU",
        "outputId": "fcbe98c1-6b43-4b3e-9912-c9877d36b4df"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
            "Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/232.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[91m‚ï∏\u001b[0m\u001b[90m‚îÅ\u001b[0m \u001b[32m225.3/232.6 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyPDF2\n",
            "Successfully installed PyPDF2-3.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install PyMuPDF"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VyGJJg4tq5gN",
        "outputId": "9d1f6588-5897-4df7-8e32-b71636e2374a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting PyMuPDF\n",
            "  Downloading pymupdf-1.26.3-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (3.4 kB)\n",
            "Downloading pymupdf-1.26.3-cp39-abi3-manylinux_2_28_x86_64.whl (24.1 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m24.1/24.1 MB\u001b[0m \u001b[31m76.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyMuPDF\n",
            "Successfully installed PyMuPDF-1.26.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pikepdf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8-ELODO4qvKn",
        "outputId": "7e34c16f-33c1-43bc-c1c7-905f7f5c0f17"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pikepdf\n",
            "  Downloading pikepdf-9.9.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (8.1 kB)\n",
            "Requirement already satisfied: Pillow>=10.0.1 in /usr/local/lib/python3.11/dist-packages (from pikepdf) (11.2.1)\n",
            "Collecting Deprecated (from pikepdf)\n",
            "  Downloading Deprecated-1.2.18-py2.py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: lxml>=4.8 in /usr/local/lib/python3.11/dist-packages (from pikepdf) (5.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from pikepdf) (24.2)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.11/dist-packages (from Deprecated->pikepdf) (1.17.2)\n",
            "Downloading pikepdf-9.9.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m41.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Deprecated-1.2.18-py2.py3-none-any.whl (10.0 kB)\n",
            "Installing collected packages: Deprecated, pikepdf\n",
            "Successfully installed Deprecated-1.2.18 pikepdf-9.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install FPDF"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V2uNmNN0q-wl",
        "outputId": "9d39baaa-61b9-486a-8e60-00bb6e2c7cc4"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting FPDF\n",
            "  Downloading fpdf-1.7.2.tar.gz (39 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: FPDF\n",
            "  Building wheel for FPDF (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for FPDF: filename=fpdf-1.7.2-py2.py3-none-any.whl size=40704 sha256=10ea6d90e66bea301ff4fc1fddb06a6ff5bb2f66506892bba7aeb8eb8fc74f72\n",
            "  Stored in directory: /root/.cache/pip/wheels/65/4f/66/bbda9866da446a72e206d6484cd97381cbc7859a7068541c36\n",
            "Successfully built FPDF\n",
            "Installing collected packages: FPDF\n",
            "Successfully installed FPDF-1.7.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rXGI4k9RqgCN"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import requests\n",
        "import time\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.parse import urljoin\n",
        "from googlesearch import search\n",
        "import fitz\n",
        "from fpdf import FPDF\n",
        "import pandas as pd\n",
        "\n",
        "OUTPUT_DIR = \"nde_dataset\"\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "# Function to download PDF from a URL\n",
        "def download_pdf(url, save_dir):\n",
        "    try:\n",
        "        if not url.lower().endswith(\".pdf\"):\n",
        "            return None\n",
        "        filename = os.path.basename(url.split(\"?\")[0])\n",
        "        filepath = os.path.join(save_dir, filename)\n",
        "        response = requests.get(url, timeout=10)\n",
        "        with open(filepath, \"wb\") as f:\n",
        "            f.write(response.content)\n",
        "        return filepath\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to download {url}: {e}\")\n",
        "        return None\n",
        "\n",
        "# Function to extract text from PDFs\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    try:\n",
        "        text = \"\"\n",
        "        doc = fitz.open(pdf_path)\n",
        "        for page in doc:\n",
        "            text += page.get_text()\n",
        "        doc.close()\n",
        "        return text\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to extract text from {pdf_path}: {e}\")\n",
        "        return \"\"\n",
        "\n",
        "# Function to scrape and extract text from a webpage\n",
        "def scrape_text_from_web(url):\n",
        "    try:\n",
        "        response = requests.get(url, timeout=10)\n",
        "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "        paragraphs = soup.find_all('p')\n",
        "        content = \"\\n\".join([p.get_text() for p in paragraphs])\n",
        "        title = soup.title.string if soup.title else \"Untitled\"\n",
        "        return title.strip(), content.strip()\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to scrape {url}: {e}\")\n",
        "        return None, None\n",
        "\n",
        "# Function to save dataset to a final PDF\n",
        "def save_dataset_to_pdf(dataset, output_pdf_path):\n",
        "    pdf = FPDF()\n",
        "    pdf.set_auto_page_break(auto=True, margin=15)\n",
        "    pdf.add_page()\n",
        "    pdf.set_font(\"Arial\", size=12)\n",
        "\n",
        "    for i, row in dataset.iterrows():\n",
        "        pdf.set_font(\"Arial\", 'B', 14)\n",
        "        pdf.multi_cell(0, 10, f\"Section: {row['title']}\")\n",
        "        pdf.set_font(\"Arial\", size=12)\n",
        "        pdf.multi_cell(0, 10, row['content'][:3000])\n",
        "        pdf.add_page()\n",
        "\n",
        "    pdf.output(output_pdf_path)\n",
        "\n",
        "# Google search for NDE content\n",
        "topics = [\"Non Destructive Evaluation\", \"Ultrasonic Testing\", \"Radiographic Testing\",\n",
        "          \"Eddy Current Testing\", \"Magnetic Particle Inspection\", \"Thermography NDE\"]\n",
        "\n",
        "dataset = []\n",
        "visited = set()\n",
        "\n",
        "for topic in topics:\n",
        "    print(f\"üîç Searching for: {topic}\")\n",
        "    links = list(search(f\"{topic} filetype:pdf OR site:org OR site:edu\", stop=50))\n",
        "    for link in links:\n",
        "        if link in visited:\n",
        "            continue\n",
        "        visited.add(link)\n",
        "        if link.lower().endswith(\".pdf\"):\n",
        "            pdf_path = download_pdf(link, OUTPUT_DIR)\n",
        "            if pdf_path:\n",
        "                content = extract_text_from_pdf(pdf_path)\n",
        "                dataset.append({\"title\": os.path.basename(pdf_path), \"content\": content})\n",
        "        else:\n",
        "            title, content = scrape_text_from_web(link)\n",
        "            if title and content:\n",
        "                dataset.append({\"title\": title, \"content\": content})\n",
        "        time.sleep(1)\n",
        "\n",
        "# Save CSV and PDF\n",
        "df = pd.DataFrame(dataset)\n",
        "df.to_csv(os.path.join(OUTPUT_DIR, \"nde_dataset.csv\"), index=False)\n",
        "save_dataset_to_pdf(df, os.path.join(OUTPUT_DIR, \"nde_dataset.pdf\"))\n",
        "\n",
        "print(\"‚úÖ Internet-wide NDE dataset created successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import requests\n",
        "import time\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.parse import urljoin\n",
        "from googlesearch import search\n",
        "import fitz  # PyMuPDF\n",
        "from fpdf import FPDF\n",
        "import pandas as pd\n",
        "import unicodedata\n",
        "import ssl\n",
        "import urllib3\n",
        "from urllib.error import HTTPError, URLError\n",
        "import socket\n",
        "\n",
        "# Disable SSL warnings\n",
        "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
        "ssl._create_default_https_context = ssl._create_unverified_context\n",
        "\n",
        "OUTPUT_DIR = \"3nde_dataset\"\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "# Normalize text for PDF\n",
        "\n",
        "def normalize_text(text):\n",
        "    try:\n",
        "        return unicodedata.normalize('NFKD', text).encode('latin-1', 'ignore').decode('latin-1')\n",
        "    except Exception:\n",
        "        return \"[Text could not be normalized]\"\n",
        "\n",
        "# Save dataset to PDF\n",
        "\n",
        "def save_dataset_to_pdf(dataset, output_pdf_path):\n",
        "    pdf = FPDF()\n",
        "    pdf.set_auto_page_break(auto=True, margin=15)\n",
        "    pdf.set_font(\"Arial\", size=12)\n",
        "\n",
        "    for i, row in dataset.iterrows():\n",
        "        if not row['content'].strip():\n",
        "            continue\n",
        "        pdf.add_page()\n",
        "        pdf.set_font(\"Arial\", 'B', 14)\n",
        "        title = normalize_text(f\"Section: {row['title']}\")\n",
        "        pdf.multi_cell(0, 10, title)\n",
        "        pdf.set_font(\"Arial\", size=12)\n",
        "        content = normalize_text(row['content'][:3000])\n",
        "        pdf.multi_cell(0, 10, content)\n",
        "\n",
        "    pdf.output(output_pdf_path)\n",
        "\n",
        "# Download PDF\n",
        "\n",
        "def download_pdf(url, save_dir):\n",
        "    try:\n",
        "        if not url.lower().endswith(\".pdf\"):\n",
        "            return None\n",
        "        filename = os.path.basename(url.split(\"?\")[0])\n",
        "        filepath = os.path.join(save_dir, filename)\n",
        "        response = requests.get(url, timeout=10, verify=False)\n",
        "        if response.status_code == 200:\n",
        "            with open(filepath, \"wb\") as f:\n",
        "                f.write(response.content)\n",
        "            return filepath\n",
        "        else:\n",
        "            print(f\"Non-200 response for {url}: {response.status_code}\")\n",
        "            return None\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to download {url}: {e}\")\n",
        "        return None\n",
        "\n",
        "# Extract text from PDF\n",
        "\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    try:\n",
        "        text = \"\"\n",
        "        doc = fitz.open(pdf_path)\n",
        "        for page in doc:\n",
        "            page_text = page.get_text()\n",
        "            if page_text:\n",
        "                text += page_text + \"\\n\"\n",
        "        doc.close()\n",
        "        return text.strip()\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to extract text from {pdf_path}: {e}\")\n",
        "        return \"\"\n",
        "\n",
        "# Scrape text from web page\n",
        "\n",
        "def scrape_text_from_web(url):\n",
        "    try:\n",
        "        headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
        "        response = requests.get(url, timeout=10, headers=headers, verify=False)\n",
        "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "        paragraphs = soup.find_all('p')\n",
        "        content = \"\\n\".join([p.get_text() for p in paragraphs if p.get_text().strip()])\n",
        "        title = soup.title.string.strip() if soup.title and soup.title.string else \"Untitled\"\n",
        "        return title, content.strip()\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to scrape {url}: {e}\")\n",
        "        return None, None\n",
        "\n",
        "# Topics to search\n",
        "topics = [\n",
        "    \"Non Destructive Evaluation\",\n",
        "    \"Ultrasonic Testing\",\n",
        "    \"Radiographic Testing\",\n",
        "    \"Eddy Current Testing\",\n",
        "    \"Magnetic Particle Inspection\",\n",
        "    \"Thermography NDE\",\n",
        "    \"Acoustic Emission Testing\",\n",
        "    \"Leak Testing NDT\",\n",
        "    \"Visual Testing NDT\",\n",
        "    \"Infrared Thermography\",\n",
        "    \"Computed Tomography NDT\",\n",
        "    \"Phased Array Ultrasonic Testing\"\n",
        "]\n",
        "\n",
        "dataset = []\n",
        "visited = set()\n",
        "\n",
        "for topic in topics:\n",
        "    print(f\"üîç Searching for: {topic}\")\n",
        "    try:\n",
        "        query = f\"{topic} filetype:pdf OR site:org OR site:edu\"\n",
        "        links = list(search(query, stop=20, pause=4))\n",
        "    except (HTTPError, URLError, socket.gaierror, socket.timeout) as e:\n",
        "        print(f\"HTTP error while searching {topic}: {e}\")\n",
        "        continue\n",
        "\n",
        "    for link in links:\n",
        "        if link in visited:\n",
        "            continue\n",
        "        visited.add(link)\n",
        "        if link.lower().endswith(\".pdf\"):\n",
        "            pdf_path = download_pdf(link, OUTPUT_DIR)\n",
        "            if pdf_path:\n",
        "                content = extract_text_from_pdf(pdf_path)\n",
        "                if content:\n",
        "                    dataset.append({\"title\": os.path.basename(pdf_path), \"content\": content})\n",
        "        else:\n",
        "            title, content = scrape_text_from_web(link)\n",
        "            if title and content:\n",
        "                dataset.append({\"title\": title, \"content\": content})\n",
        "        time.sleep(2)\n",
        "\n",
        "# Save results\n",
        "if dataset:\n",
        "    df = pd.DataFrame(dataset)\n",
        "    df = df[df['content'].str.strip().astype(bool)]\n",
        "    df.to_csv(os.path.join(OUTPUT_DIR, \"nde_dataset.csv\"), index=False)\n",
        "    save_dataset_to_pdf(df, os.path.join(OUTPUT_DIR, \"nde_dataset.pdf\"))\n",
        "    print(\"‚úÖ NDE dataset PDF and CSV saved with real content.\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No usable data collected. Consider increasing pause time.\")\n"
      ],
      "metadata": {
        "id": "rpjAWYtwvKe5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}